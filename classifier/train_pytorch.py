# -*- coding: utf-8 -*-
"""train_pytorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19is1HXvpFGaximcHjz0KCzG09CeYSyRW
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')
# %cd /content/drive/MyDrive/fake-news-detector
!pip install transformers[torch] datasets evaluate imblearn

import pandas as pd
import torch
from datasets import Dataset
from torch.optim import AdamW
from torch.utils.data import DataLoader
from tqdm.auto import tqdm
from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler

model_name = "bert-base-cased"
tokenizer_name = "bert-base-cased"
num_epochs = 5
num_classes = 2
model_directory = f'models/Pytorch-BERT'

train_data = pd.read_csv(f'dataset/news_6k_train.csv')
test_data = pd.read_csv(f'dataset/news_6k_test.csv')

hg_train_data = Dataset.from_pandas(train_data)
hg_test_data = Dataset.from_pandas(test_data)

# Length of the Dataset
print(f'The length of hg_train_data is {len(hg_train_data)}.\n')

# Check one data point
hg_train_data[0]

# Tokenizer from a pretrained model
tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)

# Take a look at the tokenizer
tokenizer

# Mapping between special tokens and their IDs.
token_info = [
    (tokenizer.unk_token, tokenizer.unk_token_id),
    (tokenizer.sep_token, tokenizer.sep_token_id),
    (tokenizer.pad_token, tokenizer.pad_token_id),
    (tokenizer.cls_token, tokenizer.cls_token_id),
    (tokenizer.mask_token, tokenizer.mask_token_id)
]

for token, token_id in token_info:
    print(f'{token}: {token_id}')

def tokenize_dataset(data):
    return tokenizer(data["text"], max_length=16, truncation=True, padding="max_length")

dataset_train = hg_train_data.map(tokenize_dataset)
dataset_test = hg_test_data.map(tokenize_dataset)

print(dataset_train)
print(dataset_test)

dataset_train[0]

# Remove the text and index columns because it will not be used in the model
columns_to_remove = ["text", "__index_level_0__"]

for column in columns_to_remove:
    try:
        dataset_train = dataset_train.remove_columns([column])
        dataset_test = dataset_test.remove_columns([column])
    except ValueError:
        pass

# Rename label to labels because the model expects the name labels
dataset_train = dataset_train.rename_column("label", "labels")
dataset_test = dataset_test.rename_column("label", "labels")

# Change the format to PyTorch tensors
dataset_train.set_format("torch")
dataset_test.set_format("torch")

print(dataset_train)
print(dataset_test)

dataset_train[0]

torch.cuda.empty_cache()

train_dataloader = DataLoader(dataset=dataset_train, shuffle=True, batch_size=16)
eval_dataloader = DataLoader(dataset=dataset_test, batch_size=16)

model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)

num_training_steps = num_epochs * len(train_dataloader)
optimizer = AdamW(params=model.parameters(), lr=5e-6)
lr_scheduler = get_scheduler(name="linear",
                             optimizer=optimizer,
                             num_warmup_steps=0,
                             num_training_steps=num_training_steps)

# Use GPU if it is available
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

# Set the progress bar
progress_bar = tqdm(range(num_training_steps))

model.train()
# Loop through the epochs
for epoch in range(num_epochs):
    # Loop through the batches
    for batch in train_dataloader:
        # Get the batch
        batch = {k: v.to(device) for k, v in batch.items()}
        # Compute the model output for the batch
        outputs = model(**batch)
        # Loss computed by the model
        loss = outputs.loss
        # backpropagates the error to calculate gradients
        loss.backward()
        # Update the model weights
        optimizer.step()
        # Learning rate scheduler
        lr_scheduler.step()
        # Clear the gradients
        optimizer.zero_grad()
        # Update the progress bar
        progress_bar.update(1)

"""### Save Model

`tokenizer.save_pretrained` saves the tokenizer information to the drive and `model.save_pretrained` saves the model.
"""

tokenizer.save_pretrained(model_directory)
model.save_pretrained(model_directory)