# -*- coding: utf-8 -*-
"""evaluate_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pKaSArmv-Di2NwD8ngI1J1FaLLqIHw4j
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')
# %cd /content/drive/MyDrive/fake-news-detector
!pip install transformers[torch] datasets evaluate imblearn

import pandas as pd
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer

import evaluate

model_directory = f'models/Tensorflow-BERT'

train_data = pd.read_csv(f'dataset/news_6k_train.csv')
test_data = pd.read_csv(f'dataset/news_6k_test.csv')

tokenizer = AutoTokenizer.from_pretrained(model_directory)
base_model = AutoModelForSequenceClassification.from_pretrained(model_directory)

hg_train_data = Dataset.from_pandas(train_data)
hg_test_data = Dataset.from_pandas(test_data)

def tokenize_dataset(data):
    return tokenizer(data["text"], max_length=16, truncation=True, padding="max_length")

dataset_train = hg_train_data.map(tokenize_dataset)
dataset_test = hg_test_data.map(tokenize_dataset)

"""## Model Evaluation

### Train Set
"""

import evaluate
import numpy as np
import pandas as pd
import tensorflow as tf

def compute_metrics(eval_pred):
    # Extract logits and labels
    logits, labels = eval_pred

    # Compute softmax probabilities and predicted labels
    probabilities = tf.nn.softmax(logits)
    predictions = np.argmax(logits, axis=1)

    # Accuracy
    accuracy_metric = evaluate.load("accuracy")
    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)

    # Precision
    precision_metric = evaluate.load("precision")
    precision = precision_metric.compute(predictions=predictions, references=labels, average='macro')

    # Recall
    recall_metric = evaluate.load("recall")
    recall = recall_metric.compute(predictions=predictions, references=labels, average='macro')

    # F1 Score
    f1_metric = evaluate.load("f1")
    f1 = f1_metric.compute(predictions=predictions, references=labels, average='macro')

    # Return metrics in a dictionary
    return {
        "eval_accuracy": accuracy['accuracy'],
        "eval_precision": precision['precision'],
        "eval_recall": recall['recall'],
        "eval_f1": f1['f1']
    }

model = Trainer(
    model=base_model,
    compute_metrics=compute_metrics
)

train_results = model.evaluate(dataset_train)

print("Training Set Evaluation:")
print("Accuracy:", train_results["eval_accuracy"])
print("Precision:", train_results["eval_precision"])
print("Recall:", train_results["eval_recall"])
print("F1:", train_results["eval_f1"])

"""### Test Set"""

model = Trainer(
    model=base_model,
    compute_metrics=compute_metrics
)

test_results = model.evaluate(dataset_test)

print("Testing Set Evaluation:")
print("Accuracy:", test_results["eval_accuracy"])
print("Precision:", test_results["eval_precision"])
print("Recall:", test_results["eval_recall"])
print("F1:", test_results["eval_f1"])