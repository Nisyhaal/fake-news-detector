# -*- coding: utf-8 -*-
"""train_tensorflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CES3hapZPyUvrtcNOhX7G57rwi8NwNfL
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')
# %cd /content/drive/MyDrive/fake-news-detector
!pip install transformers[torch] datasets evaluate imblearn

import pandas as pd
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer

model_name = "bert-base-cased"
tokenizer_name = "bert-base-cased"
num_epochs = 5
num_classes = 2
model_directory = f'models/Tensorflow-BERT'

train_data = pd.read_csv(f'dataset/news_6k_train.csv')
test_data = pd.read_csv(f'dataset/news_6k_test.csv')

hg_train_data = Dataset.from_pandas(train_data)
hg_test_data = Dataset.from_pandas(test_data)

# Length of the Dataset
print(f'The length of hg_train_data is {len(hg_train_data)}.\n')

# Check one data point
hg_train_data[0]

# Tokenizer from a pretrained model
tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)

# Take a look at the tokenizer
tokenizer

# Mapping between special tokens and their IDs.
token_info = [
    (tokenizer.unk_token, tokenizer.unk_token_id),
    (tokenizer.sep_token, tokenizer.sep_token_id),
    (tokenizer.pad_token, tokenizer.pad_token_id),
    (tokenizer.cls_token, tokenizer.cls_token_id),
    (tokenizer.mask_token, tokenizer.mask_token_id)
]

for token, token_id in token_info:
    print(f'{token}: {token_id}')

def tokenize_dataset(data):
    return tokenizer(data["text"], max_length=16, truncation=True, padding="max_length")

dataset_train = hg_train_data.map(tokenize_dataset)
dataset_test = hg_test_data.map(tokenize_dataset)

print(dataset_train)
print(dataset_test)

dataset_train[0]

base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)

# Set up training arguments
training_args = TrainingArguments(
    output_dir="./results/",
    logging_dir='./results/logs',
    logging_strategy='epoch',
    logging_steps=100,
    num_train_epochs=num_epochs,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    learning_rate=5e-6,
    seed=42,
    save_strategy='epoch',
    save_steps=100,
    evaluation_strategy='epoch',
    eval_steps=100,
    load_best_model_at_end=True
)

import evaluate
import numpy as np
import pandas as pd
import tensorflow as tf

def compute_metrics(eval_pred):
    """
    Computes evaluation metrics for a given prediction.

    Parameters:
    - eval_pred (Tuple): A tuple containing model predictions (logits) and true labels.

    Returns:
    - Dict: A dictionary containing evaluation metrics including accuracy, precision, recall, and F1 score.
    """

    # Extract logits and labels
    logits, labels = eval_pred

    # Compute softmax probabilities and predicted labels
    probabilities = tf.nn.softmax(logits)
    predictions = np.argmax(logits, axis=1)

    # Accuracy
    accuracy_metric = evaluate.load("accuracy")
    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)

    # Precision
    precision_metric = evaluate.load("precision")
    precision = precision_metric.compute(predictions=predictions, references=labels, average='macro')

    # Recall
    recall_metric = evaluate.load("recall")
    recall = recall_metric.compute(predictions=predictions, references=labels, average='macro')

    # F1 Score
    f1_metric = evaluate.load("f1")
    f1 = f1_metric.compute(predictions=predictions, references=labels, average='macro')

    # Return metrics in a dictionary
    return {
        "eval_accuracy": accuracy['accuracy'],
        "eval_precision": precision['precision'],
        "eval_recall": recall['recall'],
        "eval_f1": f1['f1']
    }

# Train the model
model = Trainer(
    model=base_model,
    args=training_args,
    train_dataset=dataset_train,
    eval_dataset=dataset_test,
    compute_metrics=compute_metrics,
    # callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]
)

model.train()

"""### Save Model

`tokenizer.save_pretrained` saves the tokenizer information to the drive and `model.save_model` saves the model.
"""

tokenizer.save_pretrained(model_directory)
model.save_model(model_directory)